{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss G: 0.4256, Train Loss D: 0.0145\n",
      "Val Loss G: 0.3678, Val Loss D: 0.0010\n",
      "Epoch 2/100\n",
      "Train Loss G: 0.3368, Train Loss D: 0.0022\n",
      "Val Loss G: 0.3299, Val Loss D: 0.0002\n",
      "Epoch 3/100\n",
      "Train Loss G: 0.3129, Train Loss D: 0.0021\n",
      "Val Loss G: 0.3545, Val Loss D: 0.0011\n",
      "Epoch 4/100\n",
      "Train Loss G: nan, Train Loss D: nan\n",
      "Val Loss G: nan, Val Loss D: nan\n",
      "Epoch 5/100\n",
      "Train Loss G: nan, Train Loss D: nan\n",
      "Val Loss G: nan, Val Loss D: nan\n",
      "Epoch 6/100\n",
      "Train Loss G: nan, Train Loss D: nan\n",
      "Val Loss G: nan, Val Loss D: nan\n",
      "Epoch 7/100\n",
      "Train Loss G: nan, Train Loss D: nan\n",
      "Val Loss G: nan, Val Loss D: nan\n",
      "Epoch 8/100\n",
      "Train Loss G: nan, Train Loss D: nan\n",
      "Val Loss G: nan, Val Loss D: nan\n",
      "Epoch 9/100\n",
      "Train Loss G: nan, Train Loss D: nan\n",
      "Val Loss G: nan, Val Loss D: nan\n",
      "Epoch 10/100\n",
      "Train Loss G: nan, Train Loss D: nan\n",
      "Val Loss G: nan, Val Loss D: nan\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data preprocessing\n",
    "def preprocess_data(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    \n",
    "    # Standardize 'Time' and 'Amount'\n",
    "    scaler = StandardScaler()\n",
    "    X[['Time', 'Amount']] = scaler.fit_transform(X[['Time', 'Amount']])\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Convert to float32\n",
    "    X_train = X_train.values.astype('float32')\n",
    "    X_test = X_test.values.astype('float32')\n",
    "    y_train = y_train.values.astype('float32')\n",
    "    y_test = y_test.values.astype('float32')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "class FeatureAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, channels):\n",
    "        super(FeatureAttention, self).__init__()\n",
    "        self.global_attention = tf.keras.layers.MultiHeadAttention(num_heads=1, key_dim=channels)\n",
    "        self.local_attention = tf.keras.layers.MultiHeadAttention(num_heads=1, key_dim=channels)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Split channels into global and local groups\n",
    "        x_global = tf.transpose(tf.reshape(x, (tf.shape(x)[0], -1, 2)), [1, 0, 2])\n",
    "        x_local = tf.transpose(tf.reshape(x, (tf.shape(x)[0], 2, -1)), [1, 0, 2])\n",
    "\n",
    "        # Apply attention\n",
    "        x_global = self.global_attention(x_global, x_global)\n",
    "        x_local = self.local_attention(x_local, x_local)\n",
    "\n",
    "        # Reshape and combine\n",
    "        x_global = tf.reshape(tf.transpose(x_global, [1, 0, 2]), tf.shape(x))\n",
    "        x_local = tf.reshape(tf.transpose(x_local, [1, 0, 2]), tf.shape(x))\n",
    "        \n",
    "        return x_global + x_local\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dims, output_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = []\n",
    "        for dim in hidden_dims:\n",
    "            self.layers.append(tf.keras.layers.Dense(dim, activation='leaky_relu'))\n",
    "        self.layers.append(tf.keras.layers.Dense(output_dim))\n",
    "\n",
    "    def call(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dims, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = []\n",
    "        for dim in hidden_dims:\n",
    "            self.layers.append(tf.keras.layers.Dense(dim, activation='leaky_relu'))\n",
    "        self.layers.append(tf.keras.layers.Dense(output_dim, activation='tanh'))\n",
    "\n",
    "    def call(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dims):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layers = []\n",
    "        for dim in hidden_dims:\n",
    "            self.layers.append(tf.keras.layers.Dense(dim, activation='leaky_relu'))\n",
    "        self.layers.append(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    def call(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class UAAD_FDNet(tf.keras.Model):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim):\n",
    "        super(UAAD_FDNet, self).__init__()\n",
    "        self.encoder = Encoder(hidden_dims, latent_dim)\n",
    "        self.decoder = Decoder(hidden_dims[::-1], input_dim)\n",
    "        self.encoder_reconstructed = Encoder(hidden_dims, latent_dim)\n",
    "        self.discriminator = Discriminator(hidden_dims)\n",
    "        self.feature_attention = FeatureAttention(input_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        x_attention = self.feature_attention(x_reconstructed)\n",
    "        z_reconstructed = self.encoder_reconstructed(x_attention)\n",
    "        return z, x_reconstructed, z_reconstructed\n",
    "\n",
    "# Loss functions\n",
    "def adversarial_loss(D_real, D_fake):\n",
    "    return -tf.reduce_mean(tf.math.log(D_real) + tf.math.log(1 - D_fake))\n",
    "\n",
    "def context_loss(x, x_reconstructed):\n",
    "    return tf.reduce_mean(tf.abs(x - x_reconstructed))\n",
    "\n",
    "def latent_loss(z, z_reconstructed):\n",
    "    return tf.reduce_mean((z - z_reconstructed) ** 2)\n",
    "\n",
    "# Training step\n",
    "@tf.function\n",
    "def train_step(model, x, optimizer_G, optimizer_D, lambda_con, lambda_lat):\n",
    "    with tf.GradientTape() as tape_G, tf.GradientTape() as tape_D:\n",
    "        z, x_reconstructed, z_reconstructed = model(x)\n",
    "        D_fake = model.discriminator(x_reconstructed)\n",
    "        D_real = model.discriminator(x)\n",
    "\n",
    "        loss_adv = adversarial_loss(D_real, D_fake)\n",
    "        loss_con = context_loss(x, x_reconstructed)\n",
    "        loss_lat = latent_loss(z, z_reconstructed)\n",
    "\n",
    "        loss_G = loss_adv + lambda_con * loss_con + lambda_lat * loss_lat\n",
    "        loss_D = adversarial_loss(D_real, D_fake)\n",
    "\n",
    "    gradients_G = tape_G.gradient(loss_G, model.trainable_variables)\n",
    "    gradients_D = tape_D.gradient(loss_D, model.discriminator.trainable_variables)\n",
    "\n",
    "    optimizer_G.apply_gradients(zip(gradients_G, model.trainable_variables))\n",
    "    optimizer_D.apply_gradients(zip(gradients_D, model.discriminator.trainable_variables))\n",
    "\n",
    "    return loss_G, loss_D\n",
    "\n",
    "# Training loop\n",
    "def train(model, train_data, val_data, num_epochs, batch_size, lr, lambda_con, lambda_lat):\n",
    "    optimizer_G = tf.keras.optimizers.Adam(lr)\n",
    "    optimizer_D = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_data).shuffle(buffer_size=1024).batch(batch_size)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(val_data).batch(batch_size)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss_G, train_loss_D = 0, 0\n",
    "        for batch in train_dataset:\n",
    "            loss_G, loss_D = train_step(model, batch, optimizer_G, optimizer_D, lambda_con, lambda_lat)\n",
    "            train_loss_G += loss_G\n",
    "            train_loss_D += loss_D\n",
    "        train_loss_G /= len(train_dataset)\n",
    "        train_loss_D /= len(train_dataset)\n",
    "\n",
    "        # Validation\n",
    "        val_loss_G, val_loss_D = 0, 0\n",
    "        for batch in val_dataset:\n",
    "            z, x_reconstructed, z_reconstructed = model(batch)\n",
    "            D_fake = model.discriminator(x_reconstructed)\n",
    "            D_real = model.discriminator(batch)\n",
    "\n",
    "            loss_adv = adversarial_loss(D_real, D_fake)\n",
    "            loss_con = context_loss(batch, x_reconstructed)\n",
    "            loss_lat = latent_loss(z, z_reconstructed)\n",
    "\n",
    "            val_loss_G += loss_adv + lambda_con * loss_con + lambda_lat * loss_lat\n",
    "            val_loss_D += adversarial_loss(D_real, D_fake)\n",
    "        val_loss_G /= len(val_dataset)\n",
    "        val_loss_D /= len(val_dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss G: {train_loss_G:.4f}, Train Loss D: {train_loss_D:.4f}\")\n",
    "        print(f\"Val Loss G: {val_loss_G:.4f}, Val Loss D: {val_loss_D:.4f}\")\n",
    "\n",
    "# Fraud detection function\n",
    "def detect_fraud(model, data, threshold):\n",
    "    z, x_reconstructed, z_reconstructed = model(data)\n",
    "    reconstruction_error = tf.reduce_mean((data - x_reconstructed) ** 2, axis=1)\n",
    "    latent_distance = tf.reduce_mean((z - z_reconstructed) ** 2, axis=1)\n",
    "    anomaly_score = reconstruction_error + latent_distance\n",
    "    predictions = tf.cast(anomaly_score > threshold, tf.float32)\n",
    "    return predictions, anomaly_score\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    input_dim = 30  # 28 PCA features + Time + Amount\n",
    "    hidden_dims = [64, 128, 256, 512]\n",
    "    latent_dim = 1024\n",
    "    batch_size = 256\n",
    "    num_epochs = 100\n",
    "    lr = 0.001\n",
    "    lambda_con = 1.0\n",
    "    lambda_lat = 0.1\n",
    "\n",
    "    # Load and preprocess data\n",
    "    data_path = \"creditcard.csv\"\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(data_path)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    model = UAAD_FDNet(input_dim, hidden_dims, latent_dim)\n",
    "    train(model, X_train, X_test, num_epochs, batch_size, lr, lambda_con, lambda_lat)\n",
    "\n",
    "    # Detect fraud on test set\n",
    "    threshold = 0.5  # This threshold should be tuned based on your specific requirements\n",
    "    test_predictions, test_scores = detect_fraud(model, X_test, threshold)\n",
    "\n",
    "    # Evaluate the model (you can add more metrics as needed)\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    accuracy = accuracy_score(y_test, test_predictions.numpy())\n",
    "    precision = precision_score(y_test, test_predictions.numpy())\n",
    "    recall = recall_score(y_test, test_predictions.numpy())\n",
    "    f1 = f1_score(y_test, test_predictions.numpy())\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
